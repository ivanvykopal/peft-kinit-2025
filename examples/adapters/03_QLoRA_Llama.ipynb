{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd72df66",
   "metadata": {},
   "source": [
    "#  3️⃣ Finetuning Llama models using Quantized-LoRA with _Adapters_\n",
    "\n",
    "In this notebook, we show how to efficiently fine-tune a quantized **Llama-3.1** model using [**QLoRA** (Dettmers et al., 2023)](https://dl.acm.org/doi/10.5555/3666122.3666563) and the [bitsandbytes](https://github.com/bitsandbytes-foundation/bitsandbytes) library.\n",
    "\n",
    "For this example, we will finetune Llama-3 8B on supervised instruction tuning data collected by the [Open Assistant project](https://github.com/LAION-AI/Open-Assistant) for training chatbots. This is similar to the setup used to train the Guanaco models in the QLoRA paper.\n",
    "You can simply replace this with any of your own domain-specific data!\n",
    "\n",
    "Additionally, you can quickly adapt this notebook to use other **adapter methods such as bottleneck adapters, prefix tuning or prompt tuning.**\n",
    "\n",
    "\n",
    "## LoRA vs. QLoRA\n",
    "\n",
    "<!-- <img src=\"../../images/qlora.png\" width=\"600\"> -->\n",
    "<img src=\"https://raw.githubusercontent.com/ivanvykopal/peft-kinit-2025/heads/master/images/qlora.png\" alt=\"LoRA vs. QLoRA\" width=\"500\"/>\n",
    "\n",
    "### How LoRA Works\n",
    "\n",
    "**LoRA** (Low-Rank Adaptation) is a parameter-efficient fine-tuning method. Instead of updating all model weights during training, LoRA freezes the original pre-trained weights and injects a small number of trainable low-rank matrices.\n",
    "\n",
    "From the image (left side):\n",
    "\n",
    "- **W (FP16)**: The original pre-trained model weights, kept frozen (non-trainable).\n",
    "- **A and B matrices**: Low-rank trainable adapters. Matrix A projects the input into a lower-dimensional space, and matrix B projects it back. Basically, we learn the difference between pre-trained and the expected trained model.\n",
    "- **D_in → D_int → D_out**: Dimensions of the input, intermediate (low-rank), and output spaces.\n",
    "- **X**: Input data, duplicated and passed through both the frozen weights and the LoRA adapters.\n",
    "\n",
    "### QLoRA\n",
    "\n",
    "**QLoRA** builds on LoRA, but applies LoRA on top of a quantized model (typically 4-bit or 8-bit). The key idea is to reduce both memory and compute requirements by combining LoRA with quantization-aware training.\n",
    "\n",
    "\n",
    "You can also open this example in Google Colab:\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ivanvykopal/peft-kinit-2025/blob/master/examples/adapters/03_QLoRA_Llama.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa993e5",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "Besides `adapters`, we require `bitsandbytes` for quantization and `accelerate` for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7bec93-c233-4493-b5d7-23d06f02d218",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq -U adapters accelerate bitsandbytes datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad77898",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64aabb0-25ef-455f-9494-b6f4ca3ecfc9",
   "metadata": {},
   "source": [
    "## Load Open Assistant dataset\n",
    "\n",
    "We use the [`timdettmers/openassistant-guanaco`](https://huggingface.co/datasets/timdettmers/openassistant-guanaco) dataset by the QLoRA, which contains a small subset of conversations from the full Open Assistant database and was also used to finetune the Guanaco models in the QLoRA paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe883fe8-868c-4cc8-92e5-ed9889143ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"timdettmers/openassistant-guanaco\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb2c4bf",
   "metadata": {},
   "source": [
    "Our training dataset has roughly 10k training samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aab5773-415c-4c90-904a-f5c0a755abfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b666c7-108b-4a64-aaec-64dfb1b10078",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[\"train\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c851ab41-4a7b-4df1-8140-48623daeae99",
   "metadata": {},
   "source": [
    "## Load and prepare model and tokenizer\n",
    "\n",
    "We download the the official Llama-3.1 8B checkpoint from the HuggingFace Hub (**Note:** You must request access to this model on the HuggingFace website and use an API token to download it.).\n",
    "\n",
    "Via the `BitsAndBytesConfig`, we specify that the model should be loaded in 4bit quantization and with double quantization for even better memory efficiency. See [their documentation](https://huggingface.co/docs/bitsandbytes/main/en/index) for more on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ef7f26-f87b-4c54-924f-c9661bc1bf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig\n",
    "\n",
    "modelpath=\"meta-llama/Llama-3.1-8B\"\n",
    "# modelpath=\"Qwen/Qwen2.5-7B\"   # Alternatively, you can use Qwen2.5-7B instead of Llama-3.1-8B\n",
    "\n",
    "# Load 4-bit quantized model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    modelpath,    \n",
    "    device_map=\"auto\",\n",
    "    quantization_config=BitsAndBytesConfig(\n",
    "        load_in_4bit=True, # we are loading the model in 4-bit\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    ),\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "model.config.use_cache = False\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelpath)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6f69bb",
   "metadata": {},
   "source": [
    "We initialize the adapter functionality in the loaded model via `adapters.init()` and add a new LoRA adapter (named `\"assistant_adapter\"`) via `add_adapter()`.\n",
    "\n",
    "In the call to `LoRAConfig()`, you can configure how and where LoRA layers are added to the model. Here, we want to add LoRA layers to all linear projections of the self-attention modules (`attn_matrices=[\"q\", \"k\", \"v\"]`) as well as intermediate and outputa linear layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6897387e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import adapters\n",
    "from adapters import LoRAConfig\n",
    "\n",
    "adapters.init(model)\n",
    "\n",
    "config = LoRAConfig(\n",
    "    selfattn_lora=True,\n",
    "    intermediate_lora=True,\n",
    "    output_lora=True,\n",
    "    attn_matrices=[\"q\", \"k\", \"v\"], # We are using LoRA on the attention matrices q, k, v\n",
    "    alpha=16,\n",
    "    r=64,\n",
    "    dropout=0.1\n",
    ")\n",
    "model.add_adapter(\"assistant_adapter\", config=config) # This is the name of the LoRA adapter we are creating\n",
    "model.train_adapter(\"assistant_adapter\")\n",
    "\n",
    "print(model.adapter_summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9eec8d5",
   "metadata": {},
   "source": [
    "To correctly train bottleneck adapters or prefix tuning, uncomment the following lines to move the adapter weights to GPU explicitly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0c26e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.adapter_to(\"assistant_adapter\", device=\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b25d99",
   "metadata": {},
   "source": [
    "Some final preparations for 4bit training: we cast a few parameters to float32 for stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ff7deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    if param.ndim == 1:\n",
    "        # cast the small parameters (e.g. layernorm) to fp32 for stability\n",
    "        param.data = param.data.to(torch.float32)\n",
    "\n",
    "# Enable gradient checkpointing to reduce required memory if needed\n",
    "# model.gradient_checkpointing_enable()\n",
    "# model.enable_input_require_grads()\n",
    "\n",
    "class CastOutputToFloat(torch.nn.Sequential):\n",
    "    def forward(self, x): return super().forward(x).to(torch.float32)\n",
    "\n",
    "model.lm_head = CastOutputToFloat(model.lm_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7211c68-07a3-4d24-a7af-a3691063a758",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24289126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying the datatypes.\n",
    "dtypes = {}\n",
    "for _, p in model.named_parameters():\n",
    "    dtype = p.dtype\n",
    "    if dtype not in dtypes:\n",
    "        dtypes[dtype] = 0\n",
    "    dtypes[dtype] += p.numel()\n",
    "total = 0\n",
    "for k, v in dtypes.items():\n",
    "    total += v\n",
    "for k, v in dtypes.items():\n",
    "    print(k, v, v / total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9913bc18-8a26-4a1b-8abc-3bc8e672f191",
   "metadata": {},
   "source": [
    "## Prepare data for training\n",
    "\n",
    "The dataset is tokenized and truncated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a941dc-9e9e-4bbd-9c2a-a54f2fd72071",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "def tokenize(element):\n",
    "    return tokenizer(\n",
    "        element[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=512, # can set to longer values such as 2048\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "\n",
    "dataset_tokenized = dataset.map(\n",
    "    tokenize, \n",
    "    batched=True, \n",
    "    num_proc=os.cpu_count(),    # multithreaded\n",
    "    remove_columns=[\"text\"]     # don't need this anymore, we have tokens from here on\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c427e534-2214-4bc6-8c73-4a81c4984db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27287f0e-56f0-458f-8c2e-9124a9739d48",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "We specify training hyperparameters and train the model using the `AdapterTrainer` class.\n",
    "\n",
    "The hyperparameters here are similar to those chosen [in the official QLoRA repo](https://github.com/artidoro/qlora/blob/main/scripts/finetune_llama2_guanaco_7b.sh), but feel free to configure as you wish!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36b9ad9-d0c0-4dc7-a4c8-03765891dac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"output/llama_qlora\",\n",
    "    # output_dir=\"output/qwen_qlora\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    save_steps=250,\n",
    "    eval_steps=187,\n",
    "    save_total_limit=3,\n",
    "    gradient_accumulation_steps=16,\n",
    "    max_steps=1000,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    learning_rate=0.0002,\n",
    "    group_by_length=True,\n",
    "    bf16=True,\n",
    "    warmup_ratio=0.03,\n",
    "    max_grad_norm=0.3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5d8b96-bcfe-490d-8462-dfb44d575432",
   "metadata": {},
   "outputs": [],
   "source": [
    "from adapters import AdapterTrainer\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "trainer = AdapterTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "    train_dataset=dataset_tokenized[\"train\"],\n",
    "    eval_dataset=dataset_tokenized[\"test\"],\n",
    "    args=args,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efbf7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe958104",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "Finally, we can prompt the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663e9b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "from transformers import logging\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "def prompt_model(model, text: str):\n",
    "    batch = tokenizer(f\"### Human: {text}\\n### Assistant:\", return_tensors=\"pt\")\n",
    "    batch = batch.to(model.device)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.inference_mode(), torch.cuda.amp.autocast():\n",
    "        output_tokens = model.generate(**batch, max_new_tokens=50)\n",
    "\n",
    "    return tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe0e9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt_model(model, \"Explain Calculus to a primary school student\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb96af2",
   "metadata": {},
   "source": [
    "## Merge LoRA weights\n",
    "\n",
    "For lower inference latency, the LoRA weights can be merged with the base model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5176ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.merge_adapter(\"assistant_adapter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403f84a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt_model(model, \"Explain NLP in simple terms\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de90e5f",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "This tutorial is inspired by the [AdapterHub](https://adapterhub.ml) project and its associated codebase. In particular, the implementation is based on the official example notebook: [**QLoRA_Llama_Finetuning.ipynb**](https://github.com/adapter-hub/adapters/blob/main/notebooks/QLoRA_Llama_Finetuning.ipynb).\n",
    "\n",
    "**Citations:**\n",
    "\n",
    "[1] Hu et al. (2021). [**LoRA: Low-Rank Adaptation of Large Language Models**](https://arxiv.org/abs/2106.09685)  \n",
    "[2] Dettmers et al. (2023). [**QLoRA: Efficient Finetuning of Quantized LLMs**](https://dl.acm.org/doi/10.5555/3666122.3666563) <br/>\n",
    "[3] [**bitsandbytes**](https://github.com/bitsandbytes-foundation/bitsandbytes) <br/>\n",
    "[4] Grattafiori et al. (2024). [**The Llama 3 Herd of Models**](https://arxiv.org/abs/2407.21783)  \n",
    "[5] Köpf et al. (2023). [**Open-Assistant**](https://arxiv.org/abs/2304.07327)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
