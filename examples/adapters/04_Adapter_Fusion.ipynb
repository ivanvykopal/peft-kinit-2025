{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYZH02h-JeX5"
      },
      "source": [
        "# 4️⃣ AdapterFusion for Sequence Classification\n",
        "\n",
        "In this example we will fine-tune the model [*BERT-base-uncased*](https://aclanthology.org/N19-1423/) to classify a sequence of tokens. For this purpose, we will use a PEFT method called **Adapter Fusion**, which creates a mixture of multiple pre-trained adapters. We will use **transformers** to download tokenizers, **datasets** for data download, **adapters** for creating adapters models and training and **evaluate** for loading evaluation metrics.\n",
        "\n",
        "You can also open this example in Google Colab:\n",
        "\n",
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ivanvykopal/peft-kinit-2025/blob/master/examples/adapters/04_Adapter_Fusion.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installation & Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jMXwq62JQ7I"
      },
      "outputs": [],
      "source": [
        "# 4.36.0 for compatibility with adapters\n",
        "# you will probably need to restart the sessions after installing these modules\n",
        "%pip install -q --user transformers[torch]==4.36.0\n",
        "%pip install -q --user datasets\n",
        "%pip install -q --user adapters\n",
        "%pip install -q --user evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import evaluate\n",
        "import logging\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    BertTokenizer,\n",
        "    BertConfig,\n",
        "    TrainingArguments,\n",
        "    default_data_collator\n",
        ")\n",
        "\n",
        "from adapters import BertAdapterModel, AdapterTrainer\n",
        "from adapters.composition import Fuse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Variable initialization\n",
        "\n",
        "We will be fine-tuning the pre-trained version of model [bert-base-uncased](https://huggingface.co/google-bert/bert-large-uncased) which has **110M** parameters. We will set the max **input length to 128** tokens and train for **3 epochs** with **batch size of 32**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = \"cuda\"\n",
        "model_name_or_path = \"bert-base-uncased\"\n",
        "tokenizer_name_or_path = \"bert-base-uncased\"\n",
        "\n",
        "max_length = 128\n",
        "lr = 1e-3\n",
        "num_epochs = 3\n",
        "batch_size = 32 # in case of \"unable to allocate\" errors, decrease the batch size to some lower number (e.g. 8 or 16)\n",
        "\n",
        "logging.disable(logging.WARNING)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The dataset that we will be using is called [Commitment Bank](https://huggingface.co/datasets/super_glue/viewer/cb) (CB) from the SuperGLUE benchmark. This dataset contains a set of premise-hypothesis pairs where the premise is a passage and the hypothesis is a clause. If the clause is contained within the passage and it is an entailment then the target is 0, for a contradiction it is 1, and 2 for a neutral clause.\n",
        "\n",
        "The dataset contains **250 training samples and 56 validation samples**. We will also split the validation part of the dataset in half to create a test part for evaluation after training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"super_glue\", \"cb\")\n",
        "\n",
        "# test set is not labeled so we need to do custom splits\n",
        "validtest = dataset[\"validation\"].train_test_split(test_size=0.5)\n",
        "\n",
        "dataset[\"validation\"] = validtest[\"train\"]\n",
        "dataset[\"test\"] = validtest[\"test\"]\n",
        "\n",
        "dataset[\"train\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we will tokenize the dataset. We only don't need to tokenize the labels this time, because we will train a classification head that returns numbers and not strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(tokenizer_name_or_path)\n",
        "\n",
        "def preprocess_function(examples):\n",
        "  return tokenizer(examples[\"premise\"], examples[\"hypothesis\"], max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "\n",
        "dataset = dataset.map(preprocess_function, batched=True)\n",
        "processed_datasets = dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    num_proc=1,\n",
        "    remove_columns=[\"premise\", \"hypothesis\", \"idx\"],\n",
        "    load_from_cache_file=False,\n",
        "    desc=\"Running tokenizer on dataset\",\n",
        ")\n",
        "\n",
        "processed_datasets = processed_datasets.rename_column(\"label\", \"labels\")\n",
        "\n",
        "train_dataset = processed_datasets[\"train\"].shuffle()\n",
        "eval_dataset = processed_datasets[\"validation\"]\n",
        "test_dataset = processed_datasets[\"test\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Adapters model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we will create the adapter model and adapter fusion. At first, we load *BertAdapterModel* using the Adapters module, and after that, we will load **3 pre-trained adapters** to the model. These adapters are pre-trained on **MNLI** (cross-genre NLI), **QQP** (question paraphrase) and **QNLI** (QA NLI). As we don't need their prediction heads, we pass **with_head=False** to the loading method.\n",
        "\n",
        "After that we will add an adapter fusion layer, that combines all 3 adapter layers, activate it and set it trainable. The *train_adapter_fusion()* does two things: It freezes all weights of the model (including adapters!) except for the fusion layer and classification head. It also activates the given adapter setup to be used in the forward pass.\n",
        "\n",
        "Here is what the AdapterFusion layer looks like in the model:\n",
        "<p align=\"center\">\n",
        "<img src=\"https://raw.githubusercontent.com/Wicwik/peft_tutorial/heads/main/img/af.png\" alt=\"adapter_fusion_arch\" width=\"auto\" height=\"350\">\n",
        "<img src=\"https://raw.githubusercontent.com/Wicwik/peft_tutorial/heads/main/img/af_arch.png\" alt=\"adapter_fusion\" width=\"auto\" height=\"350\">\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "id2label = {id: label for (id, label) in enumerate(processed_datasets[\"train\"].features[\"labels\"].names)}\n",
        "\n",
        "config = BertConfig.from_pretrained(model_name_or_path, id2label=id2label)\n",
        "\n",
        "# from transformers import AutoModelForSequenceClassification\n",
        "# model = AutoModelForSequenceClassification.from_pretrained(model_name_or_path, config=config)\n",
        "model = BertAdapterModel.from_pretrained(model_name_or_path, config=config)\n",
        "\n",
        "# comment everything from this line to the model and replace BertAdapterModel with BertForSequenceClassification to do FFT (using around 6GB of memory)\n",
        "model.load_adapter(\"nli/multinli@ukp\", load_as=\"multinli\", with_head=False)\n",
        "model.load_adapter(\"sts/qqp@ukp\", with_head=False)\n",
        "model.load_adapter(\"nli/qnli@ukp\", with_head=False)\n",
        "\n",
        "model.add_adapter_fusion(Fuse(\"multinli\", \"qqp\", \"qnli\"))\n",
        "model.set_active_adapters(Fuse(\"multinli\", \"qqp\", \"qnli\"))\n",
        "\n",
        "model.add_classification_head(\"cb\", num_labels=len(id2label))\n",
        "\n",
        "adapter_setup = Fuse(\"multinli\", \"qqp\", \"qnli\")\n",
        "model.train_adapter_fusion(adapter_setup)\n",
        "\n",
        "print(model.adapter_summary())\n",
        "\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We don't have a function for gen number of trainable parameters as in the Hugging Face PEFT module, but we can use [their implementation](https://github.com/huggingface/peft/blob/main/src/peft/peft_model.py#L492) also for this model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainable_params = 0\n",
        "all_param = 0\n",
        "for n, param in model.named_parameters():\n",
        "    num_params = param.numel()\n",
        "    if num_params == 0 and hasattr(param, \"ds_numel\"):\n",
        "        num_params = param.ds_numel\n",
        "\n",
        "    if param.__class__.__name__ == \"Params4bit\":\n",
        "        num_params = num_params * 2\n",
        "\n",
        "    all_param += num_params\n",
        "    if param.requires_grad:\n",
        "        # print(n)\n",
        "        trainable_params += num_params\n",
        "\n",
        "\n",
        "print(f\"trainable params: {trainable_params:,d} || all params: {all_param:,d} || trainable%: {100 * trainable_params / all_param}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training and Evaluation\n",
        "\n",
        "We will be using Hugging Face [TrainingArguments](https://huggingface.co/docs/transformers/v4.38.2/en/main_classes/trainer#transformers.TrainingArguments) and [AdapterTrainer](https://docs.adapterhub.ml/training.html#adaptertrainer) from Adapter Hub (this adapter is based on transformers adapter). The BERT model is not generating tokens, therefore we don't need to do any postprocessing and can use the metric form *evaluate.load()*. The trainer will take a *compute_metrics* method that will be used to compute metrics during the evaluation. \n",
        "\n",
        "For SuperGLUE CB dataset *evaluate* computes F1 and accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "metric = evaluate.load(\"super_glue\", \"cb\")\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    preds = preds.argmax(axis=1)\n",
        "\n",
        "    return metric.compute(predictions=preds, references=labels)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    \"adapters\",\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    learning_rate=lr,\n",
        "    num_train_epochs=num_epochs,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    logging_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we will do the training and evaluation. We can have a look at the memory usage.\n",
        "\n",
        "Since the *AdapterTrainer* class is inherited from the Transformers *Trainer* class, we can see the trainer uploading results to the *wandb*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for FFT you also need to replace the AdapterTrainer with standard Trainer\n",
        "# from transformers import Trainer\n",
        "# trainer = Trainer(\n",
        "#     model=model,\n",
        "#     tokenizer=tokenizer,\n",
        "#     args=training_args,\n",
        "#     train_dataset=train_dataset,\n",
        "#     eval_dataset=eval_dataset,\n",
        "#     data_collator=default_data_collator,\n",
        "#     compute_metrics=compute_metrics,\n",
        "# )\n",
        "\n",
        "trainer = AdapterTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    data_collator=default_data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "trainer.train()\n",
        "\n",
        "trainer.evaluate(eval_dataset=test_dataset, metric_key_prefix=\"test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save and load\n",
        "\n",
        "Now we can save the model with *save_adapter_fusion* and *save_all_adapters* methods to save the AdapterFusion layer and all trained adapters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "adapter_model_id = f\"{model_name_or_path}_adapterfusion_seqcls\"\n",
        "\n",
        "model.save_pretrained(adapter_model_id)\n",
        "model.save_adapter_fusion(adapter_model_id, \"multinli,qqp,qnli\")\n",
        "model.save_all_adapters(adapter_model_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can load the model and give it a custom example. It is important to **set active adapters** and to **specify the head** that we want to use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = BertAdapterModel.from_pretrained(adapter_model_id)\n",
        "model.set_active_adapters(Fuse(\"multinli\", \"qqp\", \"qnli\"))\n",
        "\n",
        "print(model.active_adapters)\n",
        "\n",
        "inputs = tokenizer(\"A pity. For myself, a great pity. But no one can say Bishop Malduin has not received latitude.\", \n",
        "                   \"Bishop Malduin has not received latitude\", \n",
        "                   return_tensors=\"pt\"\n",
        "                   )\n",
        "print(inputs)\n",
        "with torch.no_grad():\n",
        "    logits = model(**inputs, head=\"cb\")[0]\n",
        "    class_id = torch.argmax(logits).item()\n",
        "    pred_class = id2label[class_id]\n",
        "    print(pred_class, class_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References\n",
        "\n",
        "This tutorial is prepared by [Robert Belanec](https://kinit.sk/member/robert-belanec/). In particular, the implementation is based on the following example: [**adapter_fusion.ipynb**](https://github.com/Wicwik/peft_tutorial/blob/main/examples/adapter_fusion.ipynb).\n",
        "\n",
        "**Citations:**\n",
        "\n",
        "[1] Devlin et al. (2019). [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://aclanthology.org/N19-1423/) <br/>\n",
        "[2] Pfeiffer et al. (2021). [AdapterFusion: Non-Destructive Task Composition for Transfer Learning](https://aclanthology.org/2021.eacl-main.39)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
