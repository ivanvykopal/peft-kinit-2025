{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EvD_fmQGKwm7"
   },
   "source": [
    "# 1️⃣ Training an Adapter for a Transformer Model\n",
    "\n",
    "In this notebook, we will train an adapter for a **RoBERTa** ([Liu et al., 2019](https://arxiv.org/pdf/1907.11692.pdf)) model for sequence classification on a **sentiment analysis** task using the _[Adapters](https://github.com/Adapter-Hub/adapters)_ library and Hugging Face's _Transformers_ library.\n",
    "\n",
    "We will train a **[bottleneck adapter](https://docs.adapterhub.ml/methods.html#bottleneck-adapters)** on top of a pre-trained model here. Most of the code remain the same to a full fine-tuning setup using _Transformers_. The only differenece is only how the model is trained.\n",
    "\n",
    "For training, we will use the [movie review dataset by Pang and Lee (2005)](http://www.cs.cornell.edu/people/pabo/movie-review-data/). It contains movie reviews from Rotten Tomatoes which are either classified as positive or negative. We download the dataset via Hugging Face's [_Datasets_](https://github.com/huggingface/datasets) library.\n",
    "\n",
    "You can also open this example in Google Colab:\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ivanvykopal/peft-kinit-2025/blob/master/examples/adapters/01_Adapter_Training.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zcuSyuzjKzoW"
   },
   "source": [
    "## Installation\n",
    "\n",
    "First, let's install the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3P8cRBAXK1fW",
    "outputId": "f8ee713e-bb3b-4505-a28e-dec6cc18bbd1"
   },
   "outputs": [],
   "source": [
    "!pip install -qq adapters datasets accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DjiwZ8a0K7mJ"
   },
   "source": [
    "## Dataset Preprocessing\n",
    "\n",
    "Before we start to train our adapter, we need to first prepare the training data. \n",
    "\n",
    "We will use data from the [Massive Text Embedding Benchmark](https://aclanthology.org/2023.eacl-main.148/), specifically we will donwload data for sentiment classification in English.\n",
    "\n",
    "Our training dataset can be loaded via HuggingFace `datasets` using one line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LCLS9RYeK83s",
    "outputId": "f95103b4-7cd5-47d4-81ad-ac60ad2188fe"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"mteb/multilingual-sentiment-classification\", 'eng')\n",
    "dataset.num_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8DnDtZHpK-_t"
   },
   "source": [
    "Every dataset sample has an input text and a binary label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "huLjPAKHLA1g",
    "outputId": "cdeee448-f3b6-4fa1-9d93-1248c0ccddad"
   },
   "outputs": [],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sYD2HAvSLFwA"
   },
   "source": [
    "Now, we need to encode all dataset samples to valid inputs for our Transformer model. Since we want to train on `roberta-base`, we load the corresponding `RobertaTokenizer`. Using `dataset.map()`, we can pass the full dataset through the tokenizer in batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "870a8abde9324399aefa80d5cf857471",
      "f062d0cfdec94bdd82be0dbca56e77c6",
      "f85fe7b430494eb487a45ae6612d3b6d",
      "6e681b421f0f4f04af4b7d3b851f30b6",
      "ad15990f3b694fe3a79ff23f98b4aeae",
      "0747e9f532744df9bcdf8290052fd68c",
      "9cf59f42817445a3bcfec061fb6dfa94",
      "712beb84767048ac90a1267214bb6c14",
      "97788e654d604cf384bf1255a849cc95",
      "11abd146fa8f4aa4b570e0d20c0548d8",
      "621a9c99f2974b788b6b33fa2100ef20",
      "d6f0885eb18148edb987763d2040c8c0",
      "72dcd49d52c647729257f3c459aa29e7",
      "56d0b8036c4044aa9d57dea2bfe58eb8",
      "ea676a8941e74397b653fa6b9d56e777",
      "12573b73f5b74df5a9ec0913e3d1a2d5",
      "6a729356a8d3491387f03a65f99002c6",
      "d319964060cb4ac0994b663676126c14",
      "1524f853d99049b3af3d37239fa048df",
      "7a8c8a3cd6cd44c9b9aa3e0fba78d915",
      "25d3ae9d969d4cbea1f5b1ceb7e8f76e",
      "dfc9c95a46154ed8b0413473e2605354"
     ]
    },
    "id": "oDHhicggLHW7",
    "outputId": "6d31408e-c0b9-41cd-8c22-a7a43595f813"
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "\n",
    "def encode_batch(batch):\n",
    "  \"\"\"Encodes a batch of input data using the model tokenizer.\"\"\"\n",
    "  return tokenizer(batch[\"text\"], max_length=128, truncation=True, padding=\"max_length\")\n",
    "\n",
    "# Encode the input data\n",
    "dataset = dataset.map(encode_batch, batched=True)\n",
    "# The transformers model expects the target class column to be named \"labels\"\n",
    "dataset = dataset.rename_column(original_column_name=\"label\", new_column_name=\"labels\")\n",
    "# Transform to pytorch tensors and only output the required columns\n",
    "dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bwl-pv89LJfa"
   },
   "source": [
    "Now we're ready to train our model..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XgykXOBHLLFK"
   },
   "source": [
    "## Training\n",
    "\n",
    "We will use a pre-trained RoBERTa model checkpoint from the Hugging Face Hub, especially [RoBERTa Base](FacebookAI/roberta-base). We load it with [`AutoAdapterModel`](https://docs.adapterhub.ml/classes/models/auto.html), a class unique to `adapters`. In addition to regular _Transformers_ classes, this class comes with all sorts of adapter-specific functionality, allowing flexible management and configuration of multiple adapters and prediction heads. [Learn more](https://docs.adapterhub.ml/prediction_heads.html#adaptermodel-classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lENbN034LMOk",
    "outputId": "e3c7f52b-8c95-4d36-f02f-4707a623706c"
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig\n",
    "from adapters import AutoAdapterModel\n",
    "\n",
    "config = RobertaConfig.from_pretrained(\n",
    "    \"FacebookAI/roberta-base\",\n",
    "    num_labels=2,\n",
    ")\n",
    "model = AutoAdapterModel.from_pretrained(\n",
    "    \"FacebookAI/roberta-base\",\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QY4Xiy9iLOVO"
   },
   "source": [
    "**Here comes the important part!**\n",
    "\n",
    "We add a new adapter to our model by calling `add_adapter()`. We pass a name (`\"sentiment\"`) and an adapter configuration. `\"seq_bn\"` denotes a [sequential bottleneck adapter](https://docs.adapterhub.ml/methods.html#bottleneck-adapters) configuration.\n",
    "\n",
    "\n",
    "The **sequential bottleneck adapter** follows a simple yet effective architecture:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/ivanvykopal/peft-kinit-2025/heads/master/images/adapter.png\" alt=\"Sequential bottleneck adapter architecture\" width=\"400\"/>\n",
    "\n",
    "It consists of three main components arrenged sequentially:\n",
    "1. A feedforward down-projection layer, which reduces the dimensionality of the input;\n",
    "2. A non-linear activation function (typically ReLU or GELU);\n",
    "3. A feedforward up-projection layer, which projects the representation back to the original dimensionality.\n",
    "\n",
    "The output of this sequence is then added back to the original input via a residual connection, allowing the adapter to refine representations without disrupting the underlying pre-trained model. This structure enables efficient adaptation to new tasks while keeping the number of trainable parameters low.\n",
    "\n",
    "\n",
    "### Adapters library\n",
    "\n",
    "_Adapters_ library supports a diverse range of different adapter configurations. For example, `config=\"lora\"` can be passed for training a [LoRA](https://docs.adapterhub.ml/methods.html#lora) adapter, `config=\"prefix_tuning\"` for [prefix tuning](https://docs.adapterhub.ml/methods.html#prefix-tuning) or `config=\"loreft\"` for [LoReFT](https://docs.adapterhub.ml/methods.html#reft). You can find all currently supported configs [here](https://docs.adapterhub.ml/methods.html).\n",
    "\n",
    "Next, we will add a binary classification head. It's convenient to give the prediction head the same name as the adapter. This allows us to activate both together in the next step. The `train_adapter()` method does two things:\n",
    "\n",
    "1. It freezes all weights of the pre-trained model, so only the adapter weights are updated during training.\n",
    "2. It activates the adapter and the prediction head such that both are used in every forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "saGtTGogLPVf"
   },
   "outputs": [],
   "source": [
    "# Add a new adapter, in our case Sequential Bottleneck adapter (seq_bn)\n",
    "model.add_adapter(\"sentiment\", config=\"seq_bn\")\n",
    "# Alternatively, you can add different type of the adapter, e.g.:\n",
    "# model.add_adapter(\"sentiment\", config=\"lora\")\n",
    "\n",
    "# Add a matching classification head\n",
    "model.add_classification_head(\n",
    "    \"sentiment\",\n",
    "    num_labels=2,\n",
    "    id2label={ 0: \"negative\", 1: \"positive\"}\n",
    "  )\n",
    "\n",
    "# Activate the adapter for training\n",
    "model.train_adapter(\"sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zyLopENcLSEq"
   },
   "source": [
    "For training an adapter, we make use of the `AdapterTrainer` class built-in into _Adapters_. This class is largely identical to _Transformer_'s `Trainer`, with some helpful tweaks e.g. for checkpointing only adapter weights.\n",
    "\n",
    "We configure the training process using a `TrainingArguments` object and define a method that will calculate the evaluation accuracy in the end. We pass both, together with the training and validation split of our dataset, to the trainer instance.\n",
    "\n",
    "**Note the differences in hyperparameters compared to full fine-tuning.** Adapter training usually requires a few more training epochs than full fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QoTEQbhQLTGB"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import TrainingArguments, EvalPrediction\n",
    "from adapters import AdapterTrainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=4,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    logging_steps=200,\n",
    "    output_dir=\"./training_output_sentiment\",\n",
    "    overwrite_output_dir=True,\n",
    "    # The next line is important to ensure the dataset labels are properly passed to the model\n",
    "    remove_unused_columns=False,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "def compute_accuracy(p: EvalPrediction):\n",
    "  preds = np.argmax(p.predictions, axis=1)\n",
    "  return {\"acc\": (preds == p.label_ids).mean()}\n",
    "\n",
    "trainer = AdapterTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    compute_metrics=compute_accuracy,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LtSsZNJsLVk4"
   },
   "source": [
    "Start the training 🚀 (It will take approximately 6-8 minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 416
    },
    "id": "isGxmG9LLnhs",
    "outputId": "511f8d82-ef8e-47c4-8dcd-a754f4a65cfe"
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0BLPys_0LuI5"
   },
   "source": [
    "Looks good! Let's evaluate our adapter on the validation split of the dataset to see how well it learned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "id": "hnct-N6dLvOd",
    "outputId": "a0b951c7-16ac-4958-8b6d-df8298157f2d"
   },
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XTtjRB_VNJbL"
   },
   "source": [
    "We can put our trained model into a _Transformers_ pipeline to be able to make new predictions conveniently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x-BYe1jeNK98",
    "outputId": "1df24b91-cc26-4511-b496-10bae08b610f"
   },
   "outputs": [],
   "source": [
    "from transformers import TextClassificationPipeline\n",
    "\n",
    "classifier = TextClassificationPipeline(model=model, tokenizer=tokenizer, device=training_args.device.index)\n",
    "\n",
    "classifier(\"This is not awesome!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ANVQrhIsNPOu"
   },
   "source": [
    "At last, we can also extract the adapter from our model and separately save it for later reuse. Note the size difference compared to a full model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "50FhEtBrNP_C",
    "outputId": "f50f64da-cf6e-4456-de35-d0ceaebbd677"
   },
   "outputs": [],
   "source": [
    "model.save_adapter(\"./final_adapter\", \"sentiment\")\n",
    "\n",
    "!ls -lh final_adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "This tutorial was inspired by the [AdapterHub](https://adapterhub.ml) project and its associated codebase. In particular, the implementation is based on the official example notebook: [01_Adapter_Training.ipynb](https://github.com/adapter-hub/adapters/blob/main/notebooks/01_Adapter_Training.ipynb).\n",
    "\n",
    "**Citations:**\n",
    "\n",
    "[1] Muennighoff et al. (2023). [**Massive Text Embedding Benchmark (MTEB)**](https://aclanthology.org/2023.eacl-main.148/)  \n",
    "[2] Houlsby et al. (2019). [**Parameter-Efficient Transfer Learning for NLP** (Sequential Bottleneck Adapter)](https://proceedings.mlr.press/v97/houlsby19a/houlsby19a.pdf)  \n",
    "[3] Liu et al. (2019). [**RoBERTa: A Robustly Optimized BERT Pretraining Approach**](https://arxiv.org/pdf/1907.11692.pdf)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "peft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
