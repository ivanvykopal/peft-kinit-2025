{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2346159b",
   "metadata": {},
   "source": [
    "# 2️⃣ Prompt-Tuning with Autoregressive Model\n",
    "\n",
    "In this notebook, we will explore **Parameter-Efficient Fine-Tuning (PEFT)** techniques, focusing specifically on [**Prompt Tuning**](https://aclanthology.org/2021.emnlp-main.243/). \n",
    "\n",
    "---\n",
    "\n",
    "## What you'll learn:\n",
    "- The concept and mechanism of **Prompt Tuning** in LLMs:\n",
    "  - How \"soft prompts\" are optimized instead of the model weights.\n",
    "  - Why this additive approach preserves the base model and enables lightweight fine-tuning.\n",
    "- A comparison between full fine-tuning and prompt tuning in terms of compute, flexibility, and generalization.\n",
    "- Hands-on implementation of prompt tuning using Hugging Face's PEFT framework.\n",
    "\n",
    "---\n",
    "\n",
    "###  Prompt Tuning vs Fine-Tuning\n",
    "\n",
    "<!-- <img src=\"../../images/prompt-tuning.png\" width=\"600\"> -->\n",
    "<img src=\"https://raw.githubusercontent.com/ivanvykopal/peft-kinit-2025/heads/master/images/prompt-tuning.png\" alt=\"Fine-tuning vs. Prompt Tuning\" width=\"600\"/>\n",
    "\n",
    "\n",
    "The diagram illustrates that in **fine-tuning**, the model weights are adjusted for each task, often requiring full retraining and higher computational cost. In **prompt tuning**, only a small set of task-specific embeddings (the soft prompts) are learned, while the pre-trained model remains frozen. This lightweight adaptation enables multiple tasks to share the same base model with just different prompts. \n",
    "\n",
    "It is very similar to prompting, where we manually create the prompts and where we manually try to adapt the prompts to obtain the best results.\n",
    "\n",
    "---\n",
    "\n",
    "> **Key insights**:\n",
    "> - Prompt tuning adds **trainable soft prompt embeddings** to the input, leaving the model’s weights untouched.\n",
    "> - It's an additive PEFT technique that significantly lowers training and storage costs, and doesn't cause catastrophic forgetting.\n",
    "\n",
    "You can also open this example in Google Colab:\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ivanvykopal/peft-kinit-2025/blob/master/examples/peft/02_prompt_tuning.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60226d6f",
   "metadata": {},
   "source": [
    "## Step 1: Install and Import Dependencies\n",
    "\n",
    "We begin by installing required libraries and importing necessary modules. In this case, we need to install transformers, datasets and peft libraries. [PEFT](https://huggingface.co/docs/peft/en/index) library cotnains the functionality to add soft prompts to the LLM and then allow us to train only those added soft prompts.\n",
    "\n",
    "We will also set the constants and parameters for the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ccc1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q --user transformers[torch]==4.36.0\n",
    "%pip install -q --user datasets\n",
    "%pip install -q --user peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165ec9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from peft import get_peft_config, get_peft_model, PromptTuningInit, PromptTuningConfig, TaskType, PeftType\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "\n",
    "device = \"cuda\"\n",
    "dataset_name = \"twitter_complaints\"\n",
    "\n",
    "text_column = \"Tweet text\"\n",
    "label_column = \"text_label\"\n",
    "max_length = 64\n",
    "lr = 3e-2\n",
    "num_epochs = 50\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d85241",
   "metadata": {},
   "source": [
    "In this example, we will use [**BLOOMZ**](https://aclanthology.org/2023.acl-long.891/) model, which is autoregressing model supporting around 46 languages, which was trained to follow the instructions. It was one of the first models, where the model was trained on human instructions, while the model was multilingual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b1381e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"bigscience/bloomz-560m\"\n",
    "tokenizer_name_or_path = \"bigscience/bloomz-560m\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13976d2e",
   "metadata": {},
   "source": [
    "## Step 2: Load Pretrained Model and Tokenizer\n",
    "\n",
    "We load a pretrained language model and tokenizer from Hugging Face for prompt tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a3648b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"ought/raft\", dataset_name)\n",
    "\n",
    "classes = [k.replace(\"_\", \" \") for k in dataset[\"train\"].features[\"Label\"].names]\n",
    "print(classes)\n",
    "dataset = dataset.map(\n",
    "    lambda x: {\"text_label\": [classes[label] for label in x[\"Label\"]]},\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    ")\n",
    "print(dataset)\n",
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f3ee9a",
   "metadata": {},
   "source": [
    "## Step 3: Prepare Dataset\n",
    "\n",
    "Here, we load and preprocess the dataset for training and evaluation. We need to convert the texts and labels from the dataset into tokenized inputs, especially by defining input_ids and labels, which we will use later to train and evaluate the trained soft prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe12d4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "target_max_length = max([len(tokenizer(class_label)[\"input_ids\"]) for class_label in classes])\n",
    "print(target_max_length)\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    batch_size = len(examples[text_column])\n",
    "    inputs = [f\"{text_column} : {x} Label : \" for x in examples[text_column]]\n",
    "    targets = [str(x) for x in examples[label_column]]\n",
    "    model_inputs = tokenizer(inputs)\n",
    "    labels = tokenizer(targets, add_special_tokens=False)  # don't add bos token because we concatenate with inputs\n",
    "    for i in range(batch_size):\n",
    "        sample_input_ids = model_inputs[\"input_ids\"][i]\n",
    "        label_input_ids = labels[\"input_ids\"][i] + [tokenizer.eos_token_id]\n",
    "        # print(i, sample_input_ids, label_input_ids)\n",
    "        model_inputs[\"input_ids\"][i] = sample_input_ids + label_input_ids\n",
    "        labels[\"input_ids\"][i] = [-100] * len(sample_input_ids) + label_input_ids\n",
    "        model_inputs[\"attention_mask\"][i] = [1] * len(model_inputs[\"input_ids\"][i])\n",
    "    # print(model_inputs)\n",
    "    for i in range(batch_size):\n",
    "        sample_input_ids = model_inputs[\"input_ids\"][i]\n",
    "        label_input_ids = labels[\"input_ids\"][i]\n",
    "        model_inputs[\"input_ids\"][i] = [tokenizer.pad_token_id] * (\n",
    "            max_length - len(sample_input_ids)\n",
    "        ) + sample_input_ids\n",
    "        model_inputs[\"attention_mask\"][i] = [0] * (max_length - len(sample_input_ids)) + model_inputs[\n",
    "            \"attention_mask\"\n",
    "        ][i]\n",
    "        labels[\"input_ids\"][i] = [-100] * (max_length - len(sample_input_ids)) + label_input_ids\n",
    "        model_inputs[\"input_ids\"][i] = torch.tensor(model_inputs[\"input_ids\"][i][:max_length])\n",
    "        model_inputs[\"attention_mask\"][i] = torch.tensor(model_inputs[\"attention_mask\"][i][:max_length])\n",
    "        labels[\"input_ids\"][i] = torch.tensor(labels[\"input_ids\"][i][:max_length])\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "processed_datasets = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    "    load_from_cache_file=False,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")\n",
    "\n",
    "train_dataset = processed_datasets[\"train\"]\n",
    "eval_dataset = processed_datasets[\"train\"]\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True\n",
    ")\n",
    "eval_dataloader = DataLoader(eval_dataset, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641b21fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_preprocess_function(examples):\n",
    "    batch_size = len(examples[text_column])\n",
    "    inputs = [f\"{text_column} : {x} Label : \" for x in examples[text_column]]\n",
    "    model_inputs = tokenizer(inputs)\n",
    "    # print(model_inputs)\n",
    "    for i in range(batch_size):\n",
    "        sample_input_ids = model_inputs[\"input_ids\"][i]\n",
    "        model_inputs[\"input_ids\"][i] = [tokenizer.pad_token_id] * (\n",
    "            max_length - len(sample_input_ids)\n",
    "        ) + sample_input_ids\n",
    "        model_inputs[\"attention_mask\"][i] = [0] * (max_length - len(sample_input_ids)) + model_inputs[\n",
    "            \"attention_mask\"\n",
    "        ][i]\n",
    "        model_inputs[\"input_ids\"][i] = torch.tensor(model_inputs[\"input_ids\"][i][:max_length])\n",
    "        model_inputs[\"attention_mask\"][i] = torch.tensor(model_inputs[\"attention_mask\"][i][:max_length])\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "test_dataset = dataset[\"test\"].map(\n",
    "    test_preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    "    load_from_cache_file=False,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True)\n",
    "next(iter(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accc5012",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218df807",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d1fedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(test_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a78556",
   "metadata": {},
   "source": [
    "## Step 4: Configure PEFT for Prompt Tuning\n",
    "\n",
    "We configure the PEFT parameters, specifying how many prompt tokens to learn and which model layers remain frozen.\n",
    "\n",
    "Here, we defined the PrompTuning config, which describe how the soft prompt that would be added to the model should look like. In our example, we need to define the **task_type**, with CAUSAL_LM, since we are using autoregressive model.\n",
    "\n",
    "Next, we want to initialize the soft prompt with the given text and not using values from normal distribution. Since the task is classification, we will use specific text to _\"Classify if the tweet is complain or not:\"_. \n",
    "\n",
    "Lastly, we will set the number of prepended tokens to 8, but you can also choose bigger number, but it will increase the GPU memory necessary to traind the soft prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a773e092",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = PromptTuningConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    prompt_tuning_init=PromptTuningInit.TEXT,\n",
    "    num_virtual_tokens=8,\n",
    "    prompt_tuning_init_text=\"Classify if the tweet is a complaint or not:\",\n",
    "    tokenizer_name_or_path=model_name_or_path,\n",
    ")\n",
    "\n",
    "checkpoint_name = f\"{dataset_name}_{model_name_or_path}_{peft_config.peft_type}_{peft_config.task_type}_v1.pt\".replace(\n",
    "    \"/\", \"_\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715c58f5",
   "metadata": {},
   "source": [
    "## Step 5: Initialize Prompt Tuning Model\n",
    "\n",
    "We create a PEFT model by attaching learnable soft prompts to the pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6bc3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# creating model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a57752",
   "metadata": {},
   "source": [
    "## Step 6: Train the Prompt-Tuned Model\n",
    "\n",
    "Firstly, we define the optimizer and scheduler that we want to use during the training. As usually, we will use Weighted Adam optimizer.\n",
    "\n",
    "The model is trained while updating **only** the prompt embeddings, keeping the base model frozen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f91568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "# optimizer and lr scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=(len(train_dataloader) * num_epochs),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f89d5d",
   "metadata": {},
   "source": [
    "Instead of using _TrainingArguments_ and _Trainer_ from the **transformers** library, we will use our custom loop, to show you, how the basic training loop looks like.\n",
    "\n",
    "### **Training Phase**\n",
    "\n",
    "Firstly, we need to move the model to GPU to use efficient training on GPU instead of CPU.\n",
    "\n",
    "\n",
    "Then, for each epoch, we will do the following:\n",
    "1. **Set the model to training mode** with `model.train()` to enable gradient computation and dropout.\n",
    "2. **Iterate over training batches** from the `train_dataloader`.\n",
    "3. Move each batch of inputs and labels to the correct computation `device` (CPU/GPU).\n",
    "4. Perform a **forward pass**:  \n",
    "   ```python\n",
    "   outputs = model(**batch)\n",
    "   loss = outputs.loss\n",
    "   ```\n",
    "   The model computes predictions and returns the loss based on the given labels.\n",
    "5. **Accuulate loss** for monitoring purposes.\n",
    "6. **Backward pass**:\n",
    "   ```python\n",
    "   loss.backward()\n",
    "   ```\n",
    "   This computes gradients of the loss with respect to the model parameters (or soft prompts, if prompt-tuning is used).\n",
    "7. **Update parameters**:\n",
    "   ```python\n",
    "   optimizer.step()\n",
    "   lr_scheduler.step()\n",
    "   optimizer.zero_grad()\n",
    "   ```\n",
    "   The optimizer updates trainable parameters (e.g., only soft prompts in PEFT), and the learning rate scheduler adjusts the learning rate dynamically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fb69fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and evaluation\n",
    "model = model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        total_loss += loss.detach().float()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    eval_preds = []\n",
    "    for step, batch in enumerate(tqdm(eval_dataloader)):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        eval_loss += loss.detach().float()\n",
    "        eval_preds.extend(\n",
    "            tokenizer.batch_decode(torch.argmax(outputs.logits, -1).detach().cpu().numpy(), skip_special_tokens=True)\n",
    "        )\n",
    "\n",
    "    eval_epoch_loss = eval_loss / len(eval_dataloader)\n",
    "    eval_ppl = torch.exp(eval_epoch_loss)\n",
    "    train_epoch_loss = total_loss / len(train_dataloader)\n",
    "    train_ppl = torch.exp(train_epoch_loss)\n",
    "    print(f\"{epoch=}: {train_ppl=} {train_epoch_loss=} {eval_ppl=} {eval_epoch_loss=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291be0eb",
   "metadata": {},
   "source": [
    "## Step 7: Evaluate the Model\n",
    "\n",
    "Finally, we evaluate the prompt-tuned model on the validation set and analyze its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53752a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "i = 33\n",
    "inputs = tokenizer(f'{text_column} : {dataset[\"test\"][i][\"Tweet text\"]} Label : ', return_tensors=\"pt\")\n",
    "print(dataset[\"test\"][i][\"Tweet text\"])\n",
    "print(inputs)\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], max_new_tokens=10, eos_token_id=3\n",
    "    )\n",
    "    print(outputs)\n",
    "    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ec3267",
   "metadata": {},
   "source": [
    "## Step 8: Save and Load Prompt-Tuned Adapters\n",
    "\n",
    "We demonstrate how to save the learned prompt embeddings and reload them for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ba1f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving model\n",
    "peft_model_id = f\"{dataset_name}_{model_name_or_path}_{peft_config.peft_type}_{peft_config.task_type}\".replace(\n",
    "    \"/\", \"_\"\n",
    ")\n",
    "model.save_pretrained(peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4928c7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = f\"{peft_model_id}/adapter_model.bin\"\n",
    "!du -h $ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9476e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "# loading the model saved  locally\n",
    "peft_model_id = f\"{dataset_name}_{model_name_or_path}_{peft_config.peft_type}_{peft_config.task_type}\".replace(\n",
    "    \"/\", \"_\"\n",
    ")\n",
    "\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path)\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe174a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "model.eval()\n",
    "i = 4\n",
    "inputs = tokenizer(f'{text_column} : {dataset[\"test\"][i][\"Tweet text\"]} Label : ', return_tensors=\"pt\")\n",
    "print(dataset[\"test\"][i][\"Tweet text\"])\n",
    "print(inputs)\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], max_new_tokens=10, eos_token_id=3\n",
    "    )\n",
    "    print(outputs)\n",
    "    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77747e6",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "This tutorial is inspired by the code prepared within [PEFT](https://github.com/huggingface/peft) github repository. In particular, the implementation is based on the following example: [**peft_prompt_tuning_clm.ipynb**](https://github.com/huggingface/peft/blob/main/examples/causal_language_modeling/peft_prompt_tuning_clm.ipynb).\n",
    "\n",
    "\n",
    "**Citattions:**\n",
    "\n",
    "[1] Muennighoff et al. (2023). [Crosslingual Generalization through Multitask Finetuning](https://aclanthology.org/2023.acl-long.891/) <br/>\n",
    "[2] Lester et al. (2021). [The Power of Scale: Parameter-Efficient Prompt Tuning](https://aclanthology.org/2021.emnlp-main.243/) <br/>\n",
    "[3] [Hugging Face PEFT Documentation](https://huggingface.co/docs/peft/index) <br/>\n",
    "[4] [Prompt Tuning with PEFT - Hugging Face Tutorial](https://huggingface.co/learn/cookbook/en/prompt_tuning_peft) <br/>\n",
    "[5] [Fine-Tuning vs Prompt Tuning Explained](https://medium.com/@himanshu_72022/difference-between-fine-tuning-and-prompt-tuning-9f06e5d7ae11) <br/>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
