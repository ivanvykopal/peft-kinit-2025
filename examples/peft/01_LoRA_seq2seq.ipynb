{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c8ab325",
   "metadata": {},
   "source": [
    "# 1️⃣ Training a LoRA for Seq2Seq Conditional Generation\n",
    "\n",
    "In this example, we will fine-tune model `mt0-large` to generate text labels based on the input. For this purpose, we will use the reparametrization PEFT method called **Low Rank Adaptation** (LoRA). We will use transformers to download models and training, datasets for data download peft for LoRA reparametrization.\n",
    "\n",
    "You can also open this example in Google Colab:\n",
    "\n",
    "<!-- TODO: Open in Colab -->\n",
    "\n",
    "### How LoRA Works\n",
    "\n",
    "<!-- <img src=\"../../images/qlora.png\" width=\"600\"> -->\n",
    "<img src=\"https://raw.githubusercontent.com/ivanvykopal/peft-kinit-2025/heads/master/images/qlora.png\" alt=\"LoRA vs. QLoRA\" width=\"500\"/>\n",
    "\n",
    "**LoRA** (Low-Rank Adaptation) is a parameter-efficient fine-tuning method. Instead of updating all model weights during training, LoRA freezes the original pre-trained weights and injects a small number of trainable low-rank matrices.\n",
    "\n",
    "From the image (left side):\n",
    "\n",
    "- **W (FP16)**: The original pre-trained model weights, kept frozen (non-trainable).\n",
    "- **A and B matrices**: Low-rank trainable adapters. Matrix A projects the input into a lower-dimensional space, and matrix B projects it back. Basically, we learn the difference between pre-trained and the expected trained model.\n",
    "- **D_in → D_int → D_out**: Dimensions of the input, intermediate (low-rank), and output spaces.\n",
    "- **X**: Input data, duplicated and passed through both the frozen weights and the LoRA adapters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73351c4c",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "Besides `transformers`, we require `datasets` for laoding datasets and `peft` for training LoRA adapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3bbee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q --user transformers[torch]==4.36.0\n",
    "%pip install -q --user datasets\n",
    "%pip install -q --user peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a3d373",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "# Import PEFT\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    TaskType\n",
    ")\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    default_data_collator,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    GenerationConfig\n",
    ")\n",
    "\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5314a01c",
   "metadata": {},
   "source": [
    "We will be fine-tuning the pre-trained version of model `mt0-large` which has 1.2B parameters. We will set the max **input length to 128 tokens** and train for **3 epochs** with a batch size of 32.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b21054",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "model_name_or_path = \"bigscience/mt0-large\"\n",
    "tokenizer_name_or_path = \"bigscience/mt0-large\"\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "max_length = 128\n",
    "lr = 1e-3\n",
    "num_epochs = 3\n",
    "batch_size = 32 # in case of \"unable to allocate\" errors, decrease batch size to some lower number (e.g. 8 or 16) \n",
    "\n",
    "\n",
    "checkpoint_name = \"financial_sentiment_analysis_lora_v1.pt\"\n",
    "text_column = \"sentence\"\n",
    "label_column = \"text_label\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e486e52",
   "metadata": {},
   "source": [
    "The internal size of the A and B matrices will be **8 (rank)**. This means that A will have a size of **d x r** and B will have a size of **r x l**, therefore the matrix A x B will be the size of **d x l**, which is also the size of a certain matrix of weights W.\n",
    "\n",
    "We can also set the dropout rate and **alpha to scale the matrices**. We can also specify the **target modules** (names of the modules that we want to reparametrize). By default, it is set by peft library based on their known model list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cc3353",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 8 # Size of the low-rank matrices (rank)\n",
    "lora_alpha = 32 # The alpha parameter for Lora scaling\n",
    "lora_dropout = 0.1 # The dropout probability for Lora layers\n",
    "\n",
    "# Experiment with different reparametrization\n",
    "target_modules = None\n",
    "# target_modules = \"all-linear\"\n",
    "# target_modules = [\"q\", \"k\", \"v\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa143e8",
   "metadata": {},
   "source": [
    "**Now, we will create the PEFT model.**\n",
    "\n",
    "The Hugging Face PEFT module will freeze the weights and add LoRA weights automatically.\n",
    "\n",
    "Compare the model architectures with and without the added LoRA weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0850ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating model\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    inference_mode=False,\n",
    "    r=r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout\n",
    ")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07493b4c",
   "metadata": {},
   "source": [
    "We can also see that we have been able to reduce the number of trainable parameters to a mere **0.2% of original model parameters**.\n",
    "\n",
    "## Dataset and preprocessing\n",
    "\n",
    "The dataset that we will be using is called [Financial Phrasebank](https://huggingface.co/datasets/financial_phrasebank). Which is polar sentiment dataset of sentences from financial news. The dataset consists of 4.84k sentences from English language financial news categorised by sentiment. The dataset is divided by agreement rate of 5-8 annotators. We will be using the portion of dataset where all annotators agreed, which contains around **2.26k samples**.\n",
    "\n",
    "We will also split the dataset using ratio of **_80% : 10% : 10%_** for train, valid and test sets. Because we are doing seq2seq training, we also need to convert integer labels to string labels: 0 for negative, 1 for positive and 2 for neutral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee2babf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading dataset\n",
    "dataset = load_dataset(\"financial_phrasebank\", \"sentences_allagree\")\n",
    "\n",
    "dataset = dataset[\"train\"].train_test_split(test_size=0.2)\n",
    "validtest = dataset[\"test\"].train_test_split(test_size=0.5)\n",
    "\n",
    "dataset[\"validation\"] = validtest[\"train\"]\n",
    "dataset[\"test\"] = validtest[\"test\"]\n",
    "\n",
    "classes = dataset[\"train\"].features[\"label\"].names\n",
    "dataset = dataset.map(\n",
    "    lambda x: {\"text_label\": [classes[label] for label in x[\"label\"]]},\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    ")\n",
    "\n",
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cd856c",
   "metadata": {},
   "source": [
    "Now we need to tokenize the datasets. We will the tokenizer trained for the model and also tokenize the labels with padding to the max_length. The max_length for the **target labels is set to 4 tokens** (3 would be okay, but 4 is a nicer number). The padding token is 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf9608c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = examples[text_column]\n",
    "    targets = examples[label_column]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "    labels = tokenizer(targets, max_length=3, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "    labels = labels[\"input_ids\"]\n",
    "    labels[labels == tokenizer.pad_token_id] = -100\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "processed_datasets = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    "    load_from_cache_file=False,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")\n",
    "\n",
    "train_dataset = processed_datasets[\"train\"].shuffle()\n",
    "eval_dataset = processed_datasets[\"validation\"]\n",
    "test_dataset = processed_datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f11669",
   "metadata": {},
   "source": [
    "## Training and evaluation\n",
    "\n",
    "For training we are using the Hugging Face [Seq2SeqTrainer](https://huggingface.co/docs/transformers/main_classes/trainer) and provide it with [Seq2SeqTrainingArguments](https://huggingface.co/docs/transformers/v4.38.2/en/main_classes/trainer#transformers.TrainingArguments). We would also like to predict the labels during evaluation and we will do it with `model.generate()` method. The trainer will take a compute_metrics method that will be used to compute metrics during the evaluation.\n",
    "\n",
    "We would like to compute the accuracy (exact match) between two sets of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3a4090",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for pred, true in zip(preds, labels):\n",
    "        if pred.strip() == true.strip():\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    accuracy = correct / total\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    \"lora\",\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    learning_rate=lr,\n",
    "    num_train_epochs=num_epochs,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    predict_with_generate=True,\n",
    "    generation_config=GenerationConfig(max_new_tokens=10),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21af904c",
   "metadata": {},
   "source": [
    "Now we will do the training and evaluation. Give a quick look at GPU memory usage, how much are we using? How would the memory usage change if we did FFT?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cafa67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=default_data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "trainer.evaluate(eval_dataset=test_dataset, metric_key_prefix=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8748d8",
   "metadata": {},
   "source": [
    "## Save and load\n",
    "\n",
    "Now we can save the model just with the save_pretrained method (like we would for other Hugging Face transformers models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8de6005",
   "metadata": {},
   "outputs": [],
   "source": [
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "\n",
    "peft_model_id = f\"{model_name_or_path}_{peft_config.peft_type}_{peft_config.task_type}\"\n",
    "model.save_pretrained(peft_model_id)\n",
    "\n",
    "ckpt = f\"{peft_model_id}/adapter_model.safetensors\"\n",
    "!du -h $ckpt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922c68d8",
   "metadata": {},
   "source": [
    "We can now load the pre-trained model and give it a custom example.\n",
    "\n",
    "Notice that we have saved the last version of the model. In a more real scenario, we would like to save the model with the best validation score and load it at the end of the training. We can do this with training args."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c2fc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "peft_model_id = f\"{model_name_or_path}_{peft_config.peft_type}_{peft_config.task_type}\"\n",
    "\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path)\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d712ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "inputs = tokenizer(input(), return_tensors=\"pt\")\n",
    "print(inputs)\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=10)\n",
    "    print(outputs)\n",
    "    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36c3046",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "This tutorial is inspired by the code prepared by [Robert Belanec](https://kinit.sk/member/robert-belanec/). In particular, the implementation is based on the following example: [**lora_seq2seq.ipynb**](https://github.com/Wicwik/peft_tutorial/blob/main/examples/lora_seq2seq.ipynb).\n",
    "\n",
    "**Citations:**\n",
    "\n",
    "[1] Hu et al. (2021). [**LoRA: Low-Rank Adaptation of Large Language Models**](https://arxiv.org/abs/2106.09685) <br/>\n",
    "[2] [**peft**](https://github.com/huggingface/peft) <br/>\n",
    "[3] Muennighoff et al. (2022). [**Crosslingual Generalization through Multitask Finetuning**](https://arxiv.org/abs/2211.01786)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
