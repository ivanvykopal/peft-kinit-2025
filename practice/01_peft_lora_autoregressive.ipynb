{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43b50fb0",
   "metadata": {},
   "source": [
    "# 1️⃣ LoRA fine-tuning of Qwen2.5 (0.5B)\n",
    "\n",
    "**Goal:** This notebook will guide you through fine-tuning of **Qwen2.5 (0.5B)** with **LoRA** (via `peft` library) on a small custom dataset. \n",
    "\n",
    "**Notes / prerequisites:**\n",
    "\n",
    "- Runtime: choose **GPU** in Colab (preferably a T4). Free Colab GPUs have limited memory; the notebook uses 4-bit quantization and LoRA to fit.\n",
    "- This notebook contains **TODO** cells where you will implement short pieces of code.\n",
    "\n",
    "\n",
    "You can also open this example in Google Colab:\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ivanvykopal/peft-kinit-2025/blob/master/practice/01_peft_lora_autoregressive.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997f4a66",
   "metadata": {},
   "source": [
    "## Installation & Imports\n",
    "\n",
    "Run the cell below in Colab. It installs `transformers`, `accelerate`, `peft`, `bitsandbytes`, `datasets`, `safetensors`, and `evaluate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5809a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: installing in Colab may take a few minutes\n",
    "!pip install -q --upgrade pip\n",
    "!pip install -q git+https://github.com/huggingface/transformers.git\n",
    "!pip install -q accelerate bitsandbytes peft safetensors evaluate\n",
    "!pip install -q datasets==3.6.0\n",
    "\n",
    "# Optional helpful tools\n",
    "!pip install -q transformers[torch] sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb08eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check versions (useful for debugging)\n",
    "import transformers, peft, accelerate, bitsandbytes, datasets, torch\n",
    "print(\"transformers\", transformers.__version__)\n",
    "print(\"peft\", peft.__version__)\n",
    "print(\"accelerate\", accelerate.__version__)\n",
    "print(\"bitsandbytes\", bitsandbytes.__version__)\n",
    "print(\"torch\", torch.__version__)\n",
    "print(\"datasets\", datasets.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3726f930",
   "metadata": {},
   "source": [
    "## Model selection and memory strategy\n",
    "\n",
    "We'll attempt to load **Qwen2.5 (0.5B)** in 4-bit mode to fit into a free Colab GPU. \n",
    "\n",
    "**Important:** Loading in `load_in_4bit` mode requires `bitsandbytes` and may require additional CPU RAM during init."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4140ae16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Load tokenizer and model in 4-bit + prepare for LoRA\n",
    "model_name = \"Qwen/Qwen2.5-0.5B\"\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "# ensure pad token exists\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Model: load in 4-bit to fit in Colab GPU memory\n",
    "# NOTE: this will try to use bitsandbytes 4-bit quantization. If you run into issues, try loading without 4-bit\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True, # This enables 4-bit quantization\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\", # \"nf4\" is a common choice for 4-bit quantization\n",
    "#     bnb_4bit_compute_dtype=torch.float16\n",
    "# )\n",
    "\n",
    "print(\"Loading model (this may take a minute)...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    # quantization_config=bnb_config, # Here we specify the 4-bit quantization config\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model.config.use_cache = False  # required for training with some transformers versions\n",
    "print(\"Model loaded. Parameters:\", sum(p.numel() for p in model.parameters()) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b37c2b",
   "metadata": {},
   "source": [
    "## Task and dataset\n",
    "\n",
    "The `twitter_complaints` dataset contains real tweets from customers directed at various companies or services. Each tweet is annotated with a label indicating whether it contains a **complaint** or not.\n",
    "\n",
    "### Why this dataset?\n",
    "- It is realistic and practical — customer complaints are a common example of short-text classification problems in NLP.\n",
    "- It provides a supervised learning setup where, given a **prompt** (the tweet text), the model learns to predict the **label** (complaint or non-complaint).\n",
    "\n",
    "### Task definition\n",
    "Our goal is to train the model to **read a tweet and classify whether it expresses a complaint**.  \n",
    "We frame this as a **generative task** where:\n",
    "- The model receives the tweet text as the input prompt.\n",
    "- The model generates the corresponding label text (e.g., “complaint” or “non-complaint”) as the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c810b10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"ought/raft\", \"twitter_complaints\")\n",
    "print(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705d5c37",
   "metadata": {},
   "source": [
    "### Tokenization / preprocessing\n",
    "\n",
    "Participants should implement the tokenization logic below. The pattern used here is **instruction-style**: we concatenate the `input` into a prompt and use the `output` as the target continuation.\n",
    "\n",
    "**TODO**: fill `tokenize_fn` which returns `input_ids`, `attention_mask`, and `labels` (labels should be -100 for prompt tokens to avoid computing loss on them, only compute loss on target tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c29a4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Hint 1 { display-mode: \"form\" }\n",
    "\n",
    "\"\"\"\n",
    "Remember to tokenize the prompts and labels separately using the tokenizer.\n",
    "Use `tokenizer(text, max_length=max_length)` on both inputs and targets.\n",
    "Make sure to get the 'input_ids' from the tokenizer output.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309cabf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Hint 2 { display-mode: \"form\" }\n",
    "\n",
    "\"\"\"\n",
    "For each example, concatenate the prompt token IDs and label token IDs into one list.\n",
    "Add the tokenizer’s `eos_token_id` at the end of the label tokens.\n",
    "This combined list will be your final input_ids.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7f523e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Hint 3 { display-mode: \"form\" }\n",
    "\n",
    "\"\"\"\n",
    "When preparing the labels for loss computation:\n",
    "- Mask the prompt token positions with -100 to avoid computing loss on them.\n",
    "- Keep the label token IDs intact (including the eos token).\n",
    "- Pad input_ids, attention_mask, and labels to max_length with tokenizer.pad_token_id and -100 accordingly.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881a6743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement the tokenize_fn for instruction-style examples\n",
    "max_length = 256\n",
    "\n",
    "# First, we will identify which classes are presents in the datasets as the labels.\n",
    "label_names = [k.replace(\"_\", \" \") for k in dataset[\"train\"].features[\"Label\"].names]\n",
    "print(\"Label names:\", label_names)\n",
    "\n",
    "# Next, we will map the dataset to convert the labels from integers to their corresponding string names.\n",
    "dataset = dataset.map(\n",
    "    lambda x: {\"text_label\": [label_names[label] for label in x[\"Label\"]]},\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    ")\n",
    "\n",
    "# Now we will format the examples to create a prompt for each Twitter post.\n",
    "def format_example(example):\n",
    "    prompt = (\n",
    "        \"Your task is to classity Twitter posts into categories.\\n\"\n",
    "        f\"Possible categories: {', '.join(label_names)}\\n\\n\"\n",
    "        f\"Twitter post: {example['Tweet text']}\\n\"\n",
    "        \"Label:\"\n",
    "    )\n",
    "    label_text = example[\"text_label\"]\n",
    "    return {\"prompt\": prompt, \"label_text\": label_text}\n",
    "\n",
    "dataset = dataset.map(format_example)\n",
    "\n",
    "def tokenize_fn(examples):\n",
    "    \"\"\"\n",
    "    Tokenizes the input examples for the model.\n",
    "    Converts the text to input IDs and attention masks.\n",
    "    \"\"\"\n",
    "    ## --- TODO 1: YOUR CODE HERE ---\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # --- END YOUR CODE --- #\n",
    "    \n",
    "    input_ids_list = []\n",
    "    attention_mask_list = []\n",
    "    labels_list = []\n",
    "    \n",
    "    for prompt_ids, label_ids in zip(model_inputs[\"input_ids\"], labels[\"input_ids\"]):\n",
    "        ## --- TODO 2: YOUR CODE HERE ---\n",
    "        \n",
    "        \n",
    "        # --- END YOUR CODE --- #\n",
    "        \n",
    "        padding_length = max_length - len(input_ids)\n",
    "        if padding_length > 0:\n",
    "            input_ids += [tokenizer.pad_token_id] * padding_length\n",
    "            attention_mask += [0] * padding_length\n",
    "            labels += [-100] * padding_length\n",
    "        else:\n",
    "            input_ids = input_ids[:max_length]\n",
    "            attention_mask = attention_mask[:max_length]\n",
    "            labels = labels[:max_length]\n",
    "            \n",
    "        input_ids_list.append(input_ids)\n",
    "        attention_mask_list.append(attention_mask)\n",
    "        labels_list.append(labels)\n",
    "    \n",
    "    return {\n",
    "        'input_ids': input_ids_list,\n",
    "        'attention_mask': attention_mask_list,\n",
    "        'labels': labels_list\n",
    "    }\n",
    "\n",
    "# Run mapping\n",
    "tokenized_datasets = dataset.map(\n",
    "    tokenize_fn,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names # We need to remove original columns to avoid issues\n",
    ")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "# create eval split from train split and train_test_split\n",
    "train_split = tokenized_datasets[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "tokenized_datasets[\"train\"] = train_split[\"train\"]\n",
    "tokenized_datasets[\"validation\"] = train_split[\"test\"]\n",
    "\n",
    "print(\"Tokenized dataset example:\", tokenized_datasets[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c6f477",
   "metadata": {},
   "source": [
    "## LoRA configuration\n",
    "\n",
    "We'll wrap the model with `peft.get_peft_model`. PKeep `r` and `lora_alpha` small for limited GPU.\n",
    "\n",
    "**TODO**: fill `r`, `lora_alpha`, and `lora_dropout` with reasonable small values. Define alsoa the LoraConfig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b10a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Hint 1 { display-mode: \"form\" }\n",
    "\n",
    "\"\"\"\n",
    "- `r` controls the rank of the low-rank adaptation matrices; typical values are 4 or 8.\n",
    "- `lora_alpha` scales the update and is often set to 16.\n",
    "- Larger `r` can improve accuracy but increases trainable parameters and compute.\n",
    "- Choose values that balance fine-tuning quality and resource constraints.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eec36f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Hint 2 { display-mode: \"form\" }\n",
    "\n",
    "\"\"\"\n",
    "- Target modules like 'q_proj', 'v_proj', 'k_proj', and 'o_proj' correspond to attention projection layers, common targets for LoRA.\n",
    "- `lora_dropout` adds regularization; typical values are between 0.0 and 0.1.\n",
    "- Bias is often set to 'none' for causal language modeling.\n",
    "- The task type should be 'CAUSAL_LM' when fine-tuning autoregressive models.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fde5310",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# Prepare the model for k-bit training (this updates some layers to avoid weight updating issues)\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "## --- TODO 3: YOUR CODE HERE ---\n",
    "\n",
    "# --- END YOUR CODE --- #\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcb94c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Hint 1 { display-mode: \"form\" }\n",
    "\n",
    "\"\"\"\n",
    "- You can use the predefined function to check the number of trainable parameters.\n",
    "- Look at the `model.print_trainable_parameters()` method to see how many parameters are trainable.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21012b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## --- TODO 4: YOUR CODE HERE ---\n",
    "\n",
    "# --- END YOUR CODE --- #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946108f9",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "We'll use `transformers.Trainer` for simplicity. Settings are pre-defined so this can run in free Colab. You can later switch to `accelerate` + `transformers` native training for more control.\n",
    "\n",
    "**Note:** Early stopping and checkpointing are minimal here. The goal is to demonstrate training with LoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5490c8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# Data collator - for causal LM it will just pass through tensors\n",
    "def data_collator(batch):\n",
    "    import torch\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence([b['input_ids'] for b in batch], batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    attention_mask = (input_ids != tokenizer.pad_token_id).long()\n",
    "    labels = torch.nn.utils.rnn.pad_sequence([b['labels'] for b in batch], batch_first=True, padding_value=-100)\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama3_lora_results\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"no\",\n",
    "    save_strategy=\"no\",\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Small quick test: run training (this will actually fine-tune the LoRA adapters)\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d32cbba",
   "metadata": {},
   "source": [
    "## Inference / generation\n",
    "\n",
    "After fine-tuning, use the model to generate descriptions. Participants should implement the prompt and generation settings.\n",
    "\n",
    "**TODO**: experiment with generation parameters (`max_new_tokens`, `temperature`, `top_k`, `top_p`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20d73d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Hint 1 { display-mode: \"form\" }\n",
    "\n",
    "\"\"\"\n",
    "- Tokenize your prompt with `return_tensors=\"pt\"` and move inputs to the model's device.\n",
    "- Use `torch.no_grad()` during generation to avoid storing gradients.\n",
    "- Use `model.generate()` to generate text based on the input prompt.\n",
    "- Finally, decode the output tokens to text using `tokenizer.decode()` with `skip_special_tokens=True`.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d26b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Example: generate from a new input record\n",
    "def generate_from_record(record_text, max_new_tokens=60, temperature=0.7):\n",
    "    prompt = (\n",
    "        \"Your task is to classity Twitter posts into categories.\\n\"\n",
    "        f\"Possible categories: {', '.join(label_names)}\\n\\n\"\n",
    "        f\"Twitter post: {record_text}\\n\"\n",
    "        \"Label:\"\n",
    "    )\n",
    "    ## --- TODO 5: YOUR CODE HERE ---\n",
    "\n",
    "    return ...\n",
    "    # --- END YOUR CODE --- #\n",
    "\n",
    "# Test the generation function with a sample record\n",
    "sample_record = dataset[\"test\"][0][\"Tweet text\"]\n",
    "print(\"Sample record:\", sample_record)\n",
    "print(\"Generated output:\", generate_from_record(sample_record, max_new_tokens=60, temperature=0.7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6871f7",
   "metadata": {},
   "source": [
    "## Save and load LoRA adapters only\n",
    "\n",
    "To avoid saving the full base model, we save only the LoRA adapter weights (much smaller)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f949ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save adapter weights (only the PEFT/LoRA weights)\n",
    "adapter_save_path = \"./lora_adapter\"\n",
    "model.save_pretrained(adapter_save_path)\n",
    "print(\"Adapter saved to\", adapter_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f48102d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "config = PeftConfig.from_pretrained(adapter_save_path)\n",
    "\n",
    "base = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path)\n",
    "peft_model = PeftModel.from_pretrained(base, adapter_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3843e4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval\n",
    "\n",
    "for i in range(10):\n",
    "    print(f\"Example {i+1}:\")\n",
    "    sample_record = dataset[\"test\"][i][\"Tweet text\"]\n",
    "    generated_text = generate_from_record(sample_record, max_new_tokens=60)\n",
    "    print(generated_text)\n",
    "    print(\"-\" * 50)  # Separator for clarity\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
