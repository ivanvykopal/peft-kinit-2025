# üìú Papers

## Surveys

[Parameter-Efficient Fine-Tuning Methods for Pretrained Language Models: A Critical Review and Assessment](https://arxiv.org/pdf/2312.12148)

[Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey](https://arxiv.org/pdf/2403.14608)

[Parameter-Efficient Fine-Tuning in Large Models: A Survey of Methodologies](https://arxiv.org/pdf/2410.19878)

[Parameter-Efficient Fine-Tuning for Pre-Trained Vision Models: A Survey and Benchmark](https://arxiv.org/pdf/2402.02242)

## Adapters

[Parameter-Efficient Transfer Learning for NLP](https://arxiv.org/pdf/1902.00751)

[AdapterFusion: Non-Destructive Task Composition for Transfer Learning](https://aclanthology.org/2021.eacl-main.39.pdf)

## Prompt-Tuning & Prefix-Tuning

[The Power of Scale for Parameter-Efficient Prompt Tuning](https://aclanthology.org/2021.emnlp-main.243.pdf)

[Prefix-Tuning: Optimizing Continuous Prompts for Generation](https://arxiv.org/pdf/2101.00190)

## Low-Rank Adaptation (LoRA)

[LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/pdf/2106.09685)

[QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/pdf/2305.14314)

# üõ†Ô∏è Tools

[ü§ó PEFT](https://github.com/huggingface/peft)

[Adapters](https://github.com/adapter-hub/adapters)

[Unsloth](https://github.com/unslothai/unsloth)

[LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory)

[Weights & Biases](https://wandb.ai/)

[AdapterHub](https://adapterhub.ml/)

# Examples

[ü§ó PEFT - Examples](https://github.com/huggingface/peft/tree/main/examples)

[Unsloth - Examples](https://docs.unsloth.ai/get-started/unsloth-notebooks)

[Adapters - Examples](https://github.com/adapter-hub/adapters/tree/main/notebooks)
